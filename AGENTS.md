# AI Development Context & Guidelines for This Repository

> For repositories implementing a Lambda function written in Node.js, utilizing the @63klabs/cache-data package, and deployed using the Atlantis Templates and Scripts Platform by 63Klabs.

This document provides essential context for AI coding assistants working within this repository.

It outlines architectural principles, constraints, naming conventions, and deployment workflows for applications built on the **Atlantis Templates and Scripts Platform by 63Klabs** and deployed to AWS using serverless services.

All code and infrastructure generated by AI must follow these standards.

## 1. Purpose of This Repository

This repository contains the **application-specific code and infrastructure** for a serverless workload deployed on AWS.

It **does not** contain shared storage, networking, or pipeline infrastructure. Those are maintained and deployed separately.

Applications deployed via this repository follow the Golden Path for cloud development:

* Serverless-first architecture
* Event-driven design
* Modular stacks
* CI/CD pipelines per branch
* Infrastructure as Code (CloudFormation/SAM)

## 2. Platform Architecture & Guardrails

### 2.1 Atlantis Platform Templates (Do NOT Modify)

Shared templates for S3, DynamoDB, caching, networking (CloudFront/Route53), and pipelines come from:

* **Platform Template Repo:** [https://github.com/63Klabs/atlantis-cfn-template-repo-for-serverless-deployments](https://github.com/63Klabs/atlantis-cfn-template-repo-for-serverless-deployments)
* **Platform Configuration Repo:** [https://github.com/63Klabs/atlantis-cfn-configuration-repo-for-serverless-deployments](https://github.com/63Klabs/atlantis-cfn-configuration-repo-for-serverless-deployments)
* **Atlantis Application Starter 02**: [https://github.com/63Klabs/atlantis-starter-02-apigw-lambda-cache-data-nodejs](https://github.com/63Klabs/atlantis-starter-02-apigw-lambda-cache-data-nodejs)
* **63klabs/cache-data npm package**: [https://github.com/63Klabs/cache-data](https://github.com/63Klabs/cache-data)

**AI MUST NOT generate code that:**

* Rebuilds shared  infrastructure (e.g., Route53, CloudFront, global logging policies, or cross-account roles)
* Creates account-level policies or resources meant for administrators
* Deploys or manages resources, functions, or code outside of the CI/CD pipeline and deployment scripts (e.g. samconfig.toml, resource management from the CLI) (The only exception is for running development environments deploying development instances locally via SAM CLI for `DEV` environments and `dev` stages, or using AWS CLI to manage data (not infrastructure) such as values in SSM Parameter Store and Secrets Manager.)

**Upstream templates are maintained by a Platform Team**

Templates that deploy shared storage, networking, and CI/CD pipelines are maintained by a platform team and should not be duplicated.

If an application requires it's own instance of DynamoDB or S3 due to potential security or data conflicts they are able to be added to the application's template (application-infrastructure/template.yml). However, if a shared resource can be maintained, then it should be provisioned in a separate stack. A prime example is an S3 bucket hosting static web content. A single bucket can host multiple sites with proper object key organization.

The CI/CD pipeline should not be duplicated. It provides functionality and permissions for a wide variety of deployments.

If extended permissions are needed due to a CloudFormation stack deployment failing due to inadequate permissions:

* Add required permissions as **Managed Policies**
  * Create policies in a separate CloudFormation stack
  * Add the ARN of the managed policy to the appropriate pipleline parameter:
    * `CloudFormationSvcRoleIncludeManagedPolicyArns` or
    * `CodeBuildSvcRoleIncludeManagedPolicyArns`
* Do **not** edit the upstream templates.

## 3. Application Infrastructure Requirements

### 3.1 Separate Application Stacks Only

This repository **only** defines resources specific to this application.
It should *not* include:

* Shared storage buckets
* Global DynamoDB tables
* Account-level CloudFront or Route53 resources
* Organization-wide logging policies
* Any infrastructure maintained by central platform engineering

These belong in separate stacks controlled by administrators.

### 3.2 Naming Conventions (Required)

All resource names must follow:

```
<Prefix>-<ProjectId>-<StageId>-<ResourceName>
```

Where:

* **Prefix** = team or org identifier
* **ProjectId** = short identifier for the application
* **StageId** = test, beta/stage, prod
* **ResourceName** = GetPerson, AlertTopic, OrderQueue, SessionTable, etc.

**AI must respect these naming conventions in all generated example code, IAM roles, and infrastructure.**

These names will be provided to the CloudFormation template as parameters (Prefix, ProjectId, and StageId).

Correct example:

```
acme-person-api-test-GetPersonFunction
acme-schedules-prod-RefreshStepFunction
```

#### S3 Bucket Naming

Since S3 buckets must be named uniquely, and bucket names have a 63 character limit, they may follow one of two conventions:

Preferred:
```
<orgPrefix>-<Prefix>-<ProjectId>-<StageId>-<Region>-<AccountId> (orgPrefix and StageId optional)
```

* **orgPrefix** = is an optional identifier organizations can use for their buckets to ensure their uniqueness. This may be for internal teams, deployment regions, or any other use.

StageId is required if the S3 bucket is defined and provisioned within an application template. StageId is not required for templates deploying a shared (non-instance-based) resource.

Alternate using a resource name if multiple buckets are produced by a single template:
```
<orgPrefix>-<Prefix>-<ProjectId>-<StageId>-<ResourceName> (orgPrefix required, StageId optional)
```

In this case the orgPrefix should be unique to take place of the Region and AccountId requirement. However, if not using an orgPrefix:
```
<Prefix>-<ProjectId>-<StageId>-<ResourceName>-<Region>-<AccountId> (orgPrefix required, StageId optional)
```

#### SSM Parameter Naming

An application can make use of any SSM Parameter it has permission to access. However, for SSM Parameters created specifically for an application instance, it should be named in the following format:

```
/<OptionalOrgPath>/<ENV>/<Prefix>-<ProjectId>-<StageId>/<ParamName>
```

For example:
```
/sam-apps/TEST/acme-order-processing-test/RemoteApiKey
```

Note: CodeBuild already has the application's base hierarchy available in the environment variable `PARAM_STORE_HIERARCHY` which is also available in the application's CloudFormation template as `ParameterStoreHierarchy`. These can be used in scripts and Lambda functions to prevent having to generate the path on your own. `${PARAM_STORE_HIERARCHY}RemoteApiKey`


### 3.3 IAM Policies – Principle of Least Privilege

AI must follow these rules when generating IAM policies:

* **Never** use AWS managed policies such as `AWSLambdaFullAccess`, `AmazonS3FullAccess`, or `LambdaAll`.
* Always generate **tight, resource-scoped** permissions using ARNs that follow the naming convention.
* Policies must limit both **actions** and **resources**.

Correct:

```yaml
Action:
  - s3:PutObject
Resource:
  - !Sub arn:aws:s3:::${Prefix}-${ProjectId}-${StageId}-output/* 
```

Incorrect:

```yaml
Action: s3:*
Resource: "*"
```

## 4. Deployment Workflows & Branching Model

### 4.1 Branch-to-Environment Mapping

The following branches correspond to automated AWS Pipelines:

| Branch   | Environment | Stage       |
| -------- | ----------- | ----------- |
| **test** | TEST        | Test        |
| **beta** | PROD        | Beta        |
| **stage** | PROD       | Stage       |
| **main** | PROD        | Prod        |

The use/naming of a beta or staging branch is up to the project owner. However, when deployed, it functions like a production environment.

Production environments (PROD) will have gradual deployments, longer log retention, and CloudWatch Alarms and Dashboards.

Non-production environments (TEST) will have immediate deployments, shorter log retention, more verbose logs, and fewer resources that could incur costs that are only needed at the production level (such as Alarms and Dashboards).

Merge sequence:

```
dev → test → beta → main
```

AI-generated instructions must follow this workflow.

> Additional branches and pipelines may be set up. For example a test feature branch and pipeline may be test-feat89 and tf89 respectively.

### 4.2 Deployment Requirements

* Deployments are done **exclusively** through Atlantis Platform Pipeline Templates.
* Infrastructure changes must be deployed through the shared CI/CD pipeline—not manual deployments.
* Conditional logic may be built into the template and any environment variables based on the Deployment Environment (TEST/PROD)
* Scripts to be run during the CodeBuild phase should be stored in application-infrastructure/build-scripts and called from buildspec.yml.

AI suggestions **must not** recommend:

* Manual AWS Console deployments
* Creating custom pipeline YAML/JSON
* Editing CodePipeline templates
* Using Terraform, CDK, or serverless framework

The Cloud Development Kit (CDK) should be reserved for complex builds involving variable resources and automated provisioning. Any deployment occuring via CDK should have a script or process that not only facilitates the deployment, but also the tear-down and clean-up of all resources generated. Resource tags used by the CloudFormation stack that deploys the application must be propagated among all resources generated by the CDK.

The same goes for any dynamic resources generated and maintained by the application using the AWS SDK.

## 5. Architectural Guidelines for AI-Generated Code

### 5.1 Prefer AWS Native Services

AI should suggest using AWS-managed services before writing custom code:

* EventBridge
* S3 event notifications
* SNS / SQS
* Step Functions
* DynamoDB
* API Gateway

Avoid large monolithic libraries; prefer event-driven, loosely coupled services.

### 5.2 Minimize Dependencies

AI must follow these rules:

* Do not package the AWS SDK in production environment (already available in Lambda)
* Favor native language libraries
  * Node.js built-in `crypto`
  * Python `hashlib`, `datetime`, `json`, etc.
* Avoid large third-party libraries unless absolutely necessary
* Utilize functionality in the @63klabs/cache-data package for Node.js

### 5.3 Code Quality & Testing

AI-generated functions or methods should include:

* Unit test scaffolding
* Meaningful test cases
* Mocking of AWS SDK calls using Jest for testing
* Clear separation of business logic from handler code

## 6. Security Requirements

### AI must always prioritize security by default:

* Use encryption at rest and in transit (KMS, HTTPS, S3 defaults)
* Enforce least privilege
* Ensure no secrets or credentials appear in code or logs
* Recommend Secrets Manager or SSM Parameter Store
* Avoid hardcoding environment variables that contain sensitive data
* Validate all inputs (API or event)
* Sanitize logs

## 7. What AI Tools Should Assume About This Repository

AI tools should assume:

* This application runs in a **serverless**, **event-driven** model
* Infrastructure is defined via CloudFormation/SAM
* CI/CD is fully automated
* Naming conventions and permissions are strictly enforced
* The application will be long-lived and require maintainability and cost efficiency

AI should **not** assume:

* Use of EC2, EKS, or persistent servers
* Unrestricted IAM roles
* Direct access to VPC-bound enterprise systems unless explicitly provided
* Experimental architectures that break platform guardrails

## 8. If AI Generates New Resources

It must:

* Fit within a modular architecture
* Follow naming conventions
* Respect the separation of stacks
* Avoid privileged or global resources
* Include appropriate IAM scoping
* Be consistent across test/beta/prod

## 9. Summary for AI (TL;DR)

* Use **serverless**, **event-driven**, **modular** architectures.
* Follow strict naming: `Prefix-ProjectId-StageId-*`.
* Apply **least privilege** IAM policies—never AWS-managed policies.
* Never modify Atlantis platform templates.
* Deploy only through pipeline scripts; no manual deployments.
* Keep dependencies small; use AWS-native services.
* Include unit tests for new functions.
* Keep shared infrastructure out of this repo.
